{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "8b202b33-1ee0-4aac-87d9-acffba697491",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from datasketch import MinHash, MinHashLSH\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "7245f0a3-ca39-450a-bfba-35c02322db47",
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "8db59466-8d5e-40ed-84ab-85d7d46221d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "nol = pd.read_csv(r'C:\\Users\\Bill\\Desktop\\texnikes analisis\\test_without_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "5c375556-1e6c-4f35-8162-4b7f2bae2c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(r'C:\\Users\\Bill\\Desktop\\texnikes analisis\\train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "b8f830f9-2dc0-46e3-8bf6-f60ddca71130",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "0c5a0b5c-e7a7-4fba-86ab-68ed821685b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nol = nol.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "b3bbcdea-0a6f-4832-896e-bff584cc8f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(title, content):\n",
    "    \"\"\"Combine title and content, then tokenize into a set of words.\"\"\"\n",
    "    combined_text = f\"{title} {content}\".lower()  # Combine and lowercase\n",
    "    tokens = word_tokenize(combined_text)         # Tokenize into words\n",
    "    return set(tokens)                            # Convert to a set (unique words)\n",
    "\n",
    "def jaccard_similarity(set1, set2):\n",
    "    intersection = len(set1 & set2)  # Elements common to both sets\n",
    "    union = len(set1 | set2)         # Unique elements in both sets\n",
    "    return intersection / union if union != 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "ea4be424-d6db-4bfb-bcd9-6535281ceec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def brute_force_knn(train_df, test_df, k=7):\n",
    "    # Create copies of the input DataFrames\n",
    "    train_df_copy = train_df.copy()\n",
    "    test_df_copy = test_df.copy()\n",
    "\n",
    "    # Tokenization step\n",
    "    train_df_copy[\"tokenized\"] = train_df_copy.apply(\n",
    "        lambda row: preprocess_text(row[\"Title\"], row[\"Content\"]), axis=1\n",
    "    )\n",
    "    test_df_copy[\"tokenized\"] = test_df_copy.apply(\n",
    "        lambda row: preprocess_text(row[\"Title\"], row[\"Content\"]), axis=1\n",
    "    )\n",
    "\n",
    "    predicted_labels = []\n",
    "    neighbor_ids_list = []\n",
    "\n",
    "    start_query_time = time.time()\n",
    "\n",
    "    for _, test_row in tqdm(test_df_copy.iterrows(), total=len(test_df_copy)):\n",
    "        test_tokens = test_row[\"tokenized\"]\n",
    "        similarities = []\n",
    "\n",
    "        for _, train_row in train_df_copy.iterrows():\n",
    "            train_tokens = train_row[\"tokenized\"]\n",
    "            similarity = jaccard_similarity(test_tokens, train_tokens)\n",
    "            similarities.append((train_row[\"Id\"], train_row[\"Label\"], similarity))\n",
    "\n",
    "        top_k_neighbors = sorted(similarities, key=lambda x: x[2], reverse=True)[:k]\n",
    "\n",
    "        neighbor_ids = {neighbor[0] for neighbor in top_k_neighbors}\n",
    "        neighbor_ids_list.append(neighbor_ids)\n",
    "\n",
    "        neighbor_labels = [neighbor[1] for neighbor in top_k_neighbors]\n",
    "\n",
    "        most_common_label = Counter(neighbor_labels).most_common(1)[0][0]\n",
    "\n",
    "        predicted_labels.append(most_common_label)\n",
    "\n",
    "    query_time = time.time() - start_query_time\n",
    "\n",
    "    test_df_copy[\"PredictedLabel\"] = predicted_labels\n",
    "    test_df_copy[\"NeighborIds\"] = neighbor_ids_list\n",
    "\n",
    "    print(f\"Query time: {query_time:.2f} seconds\")\n",
    "\n",
    "    return test_df_copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "4c3de693-e945-4ff9-b849-ae9bcff11bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_with_minhash_lsh(train_df, test_df, k=7, threshold=0.9, num_perm=16):\n",
    "    # Create copies of the input DataFrames\n",
    "    train_df_copy = train_df.copy()\n",
    "    test_df_copy = test_df.copy()\n",
    "\n",
    "    # Step 1: Preprocess and tokenize the text data\n",
    "    train_df_copy[\"tokenized\"] = train_df_copy.apply(\n",
    "        lambda row: preprocess_text(row[\"Title\"], row[\"Content\"]), axis=1\n",
    "    )\n",
    "    test_df_copy[\"tokenized\"] = test_df_copy.apply(\n",
    "        lambda row: preprocess_text(row[\"Title\"], row[\"Content\"]), axis=1\n",
    "    )\n",
    "\n",
    "    # Step 2: Create MinHash objects for the training data\n",
    "    start_build_time = time.time()\n",
    "    train_df_copy[\"minhash\"] = train_df_copy[\"tokenized\"].apply(\n",
    "        lambda tokens: create_minhash(tokens, num_perm=num_perm)\n",
    "    )\n",
    "\n",
    "    # Step 3: Initialize LSH\n",
    "    lsh = MinHashLSH(threshold=threshold, num_perm=num_perm)\n",
    "    for idx, minhash in enumerate(train_df_copy[\"minhash\"]):\n",
    "        lsh.insert(train_df_copy.iloc[idx][\"Id\"], minhash)  # Use the \"Id\" column as a unique key\n",
    "    build_time = time.time() - start_build_time\n",
    "\n",
    "    predicted_labels = []\n",
    "    neighbor_ids_list = []\n",
    "\n",
    "    # Step 4: Query the LSH for each test instance\n",
    "    start_query_time = time.time()\n",
    "    for _, test_row in tqdm(test_df_copy.iterrows(), total=len(test_df_copy)):\n",
    "        test_tokens = test_row[\"tokenized\"]\n",
    "        test_minhash = create_minhash(test_tokens, num_perm=num_perm)\n",
    "        approximate_neighbors = lsh.query(test_minhash)  # Retrieve approximate neighbors\n",
    "\n",
    "        neighbor_ids_list.append(set(map(int, approximate_neighbors)))\n",
    "        \n",
    "        if approximate_neighbors:\n",
    "            # Find corresponding labels for the neighbors\n",
    "            neighbor_labels = train_df_copy[\n",
    "                train_df_copy[\"Id\"].isin(approximate_neighbors)\n",
    "            ][\"Label\"]\n",
    "\n",
    "            # Take the top-k most common labels\n",
    "            most_common_label = Counter(neighbor_labels).most_common(1)[0][0]\n",
    "        else:\n",
    "            # If no neighbors are found, assign a default or random label\n",
    "            most_common_label = \"default_label\"  # Replace with an actual default strategy\n",
    "\n",
    "        predicted_labels.append(most_common_label)\n",
    "    query_time = time.time() - start_query_time\n",
    "\n",
    "    # Add predictions and neighbors to the test DataFrame\n",
    "    test_df_copy[\"PredictedLabel\"] = predicted_labels\n",
    "    test_df_copy[\"NeighborIds\"] = neighbor_ids_list\n",
    "\n",
    "    print(f\"Build time: {build_time:.2f} seconds\")\n",
    "    print(f\"Query time: {query_time:.2f} seconds\")\n",
    "\n",
    "    return test_df_copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "153e8f98-8652-4b49-a24e-30947c67d679",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:03<00:00, 25.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query time: 3.92 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "result_df = brute_force_knn(train, nol, k=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "1eac5c17-de24-4b96-82ff-05b500967dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 147.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build time: 6.90 seconds\n",
      "Query time: 0.68 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lsh_result_df = knn_with_minhash_lsh(train, nol, k=7, threshold=0.3, num_perm=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "437586e2-6730-4c18-88dd-2456eea89146",
   "metadata": {},
   "outputs": [],
   "source": [
    "joint = result_df.merge(lsh_result_df, on = 'Id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "14063f47-e6f9-4a0f-bfa3-a865b2652416",
   "metadata": {},
   "outputs": [],
   "source": [
    "joint[\"CommonNeighborCount\"] = joint.apply(\n",
    "    lambda row: len(row[\"NeighborIds_x\"].intersection(row[\"NeighborIds_y\"])),\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "c7a9bfad-fcd1-4d90-bca3-c8bc1a729a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.571428571428571 %\n"
     ]
    }
   ],
   "source": [
    "print((joint.CommonNeighborCount.sum()/(7*len(joint)))*100,'''%''')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
